{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef9f405-0785-413c-a7ed-32156eb80499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Physical Cores: 18\n",
      "CPU Logical Cores: 36\n",
      "Total RAM: 134721097728\n",
      "\n",
      "GPU Information:\n",
      "GPU ID: 0\n",
      "GPU Name: NVIDIA GeForce RTX 3090\n",
      "Total GPU Memory: 24576.0\n",
      "Used GPU Memory: 276.0\n",
      "Free GPU Memory: 23983.0\n",
      "GPU Temperature: 29.0\n",
      "GPU Load: 0.0\n",
      "\n",
      "\n",
      "GPU ID: 1\n",
      "GPU Name: NVIDIA GeForce RTX 3090\n",
      "Total GPU Memory: 24576.0\n",
      "Used GPU Memory: 138.0\n",
      "Free GPU Memory: 24113.0\n",
      "GPU Temperature: 33.0\n",
      "GPU Load: 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "# CPU\n",
    "cpu_count = psutil.cpu_count(logical=False)  # Physical CPU count\n",
    "cpu_logical_count = psutil.cpu_count(logical=True)  # Logical CPU count\n",
    "\n",
    "# RAM\n",
    "total_memory = psutil.virtual_memory().total\n",
    "\n",
    "# GPU\n",
    "gpus = GPUtil.getGPUs()\n",
    "gpu_info = []\n",
    "for gpu in gpus:\n",
    "    gpu_info.append({\n",
    "        'id': gpu.id,\n",
    "        'name': gpu.name,\n",
    "        'memory_total': gpu.memoryTotal,\n",
    "        'memory_used': gpu.memoryUsed,\n",
    "        'memory_free': gpu.memoryFree,\n",
    "        'temperature': gpu.temperature,\n",
    "        'load': gpu.load\n",
    "    })\n",
    "\n",
    "print(\"CPU Physical Cores:\", cpu_count)\n",
    "print(\"CPU Logical Cores:\", cpu_logical_count)\n",
    "print(\"Total RAM:\", total_memory)\n",
    "\n",
    "print(\"\\nGPU Information:\")\n",
    "for gpu in gpu_info:\n",
    "    print(\"GPU ID:\", gpu['id'])\n",
    "    print(\"GPU Name:\", gpu['name'])\n",
    "    print(\"Total GPU Memory:\", gpu['memory_total'])\n",
    "    print(\"Used GPU Memory:\", gpu['memory_used'])\n",
    "    print(\"Free GPU Memory:\", gpu['memory_free'])\n",
    "    print(\"GPU Temperature:\", gpu['temperature'])\n",
    "    print(\"GPU Load:\", gpu['load'])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887f8426-322b-43da-adbd-51f789a8c67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gputil in /home/mahin/anaconda3/lib/python3.11/site-packages (1.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gputil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f792169e-c791-45b3-9b81-a74496a16113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Model: NVIDIA GeForce RTX 3090\n",
      "GPU ID: 0\n",
      "GPU Memory Total: 24576.0 MB\n",
      "GPU Memory Free: 23983.0 MB\n",
      "GPU Memory Used: 276.0 MB\n",
      "GPU Driver: 535.161.07\n",
      "\n",
      "==============================\n",
      "\n",
      "GPU Model: NVIDIA GeForce RTX 3090\n",
      "GPU ID: 1\n",
      "GPU Memory Total: 24576.0 MB\n",
      "GPU Memory Free: 24113.0 MB\n",
      "GPU Memory Used: 138.0 MB\n",
      "GPU Driver: 535.161.07\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "\n",
    "# Get the list of available GPUs\n",
    "gpus = GPUtil.getGPUs()\n",
    "\n",
    "# Print information for each GPU\n",
    "for gpu in gpus:\n",
    "    print(f\"GPU Model: {gpu.name}\")\n",
    "    print(f\"GPU ID: {gpu.id}\")\n",
    "    print(f\"GPU Memory Total: {gpu.memoryTotal} MB\")\n",
    "    print(f\"GPU Memory Free: {gpu.memoryFree} MB\")\n",
    "    print(f\"GPU Memory Used: {gpu.memoryUsed} MB\")\n",
    "    print(f\"GPU Driver: {gpu.driver}\")\n",
    "    print(\"\\n\" + \"=\"*30 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b00fc08d-156e-48f6-be90-524a6ee82fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /home/mahin/anaconda3/lib/python3.11/site-packages (5.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b54b83d-881e-4afb-a908-b1c0a8805272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 125.47 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get the total RAM in GB\n",
    "ram_gb = psutil.virtual_memory().total / (1024 ** 3)\n",
    "\n",
    "print(f\"Total RAM: {ram_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b38f0814-8288-478d-b324-95e99e1d7ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /home/mahin/Downloads/cifake\n",
      "Contents of the Downloads folder:\n",
      "train\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the path to the Downloads folder\n",
    "dataset_dir = os.path.expanduser('~/Downloads/cifake')\n",
    "\n",
    "print(\"Loading dataset from: \" + dataset_dir)\n",
    "\n",
    "# List all files and directories in the Downloads folder\n",
    "folder_contents = os.listdir(dataset_dir)\n",
    "\n",
    "# Store the items in a list\n",
    "folder_items = []\n",
    "\n",
    "for item in folder_contents:\n",
    "    folder_items.append(item)\n",
    "\n",
    "# Print the list of files and directories\n",
    "print(\"Contents of the Downloads folder:\")\n",
    "for item in folder_items:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6c61223-8435-428b-b297-354d9f493526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from IPython.display import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe13c06c-3cc4-44f8-92c1-de54566ac81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASD\n"
     ]
    }
   ],
   "source": [
    "print(\"ASD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f29d1d3-e283-4f75-999e-6b6f98fc0182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 2 classes.\n",
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "img_height = 32\n",
    "img_width = 32\n",
    "batch_size = 500\n",
    "\n",
    "# Load the training data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_ds = train_datagen.flow_from_directory(\n",
    "  dataset_dir + \"/train\",\n",
    "  seed = 512,\n",
    "  target_size = (img_height, img_width),\n",
    "  batch_size = batch_size)\n",
    "\n",
    "# Load the validation data\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_ds = val_datagen.flow_from_directory(\n",
    "  dataset_dir + \"/test\",\n",
    "  seed = 512,\n",
    "  target_size = (img_height, img_width),\n",
    "  batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744a3808-5426-4099-898b-72f3070fc9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting the error rate and metrics rate\n",
    "def plot_metrics(history, metric):\n",
    "    plt.plot(history.history[metric], label = metric)\n",
    "    plt.plot(history.history['val_' + metric], label='val_' + metric)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Constant values that will be shared by all the models\n",
    "val_true_classes = np.concatenate([y for x, y in val_ds], axis = 0)  # Get true labels\n",
    "class_names = ['FAKE', 'REAL']\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 10, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b5263f-dae5-4b6c-9728-413393978577",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ASD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88884e4c-75d1-4d4c-8f7d-ec23614f9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Transfer Learning model using ResNet50\n",
    "ResNet_base_model = tf.keras.applications.ResNet50(\n",
    "    include_top = False, \n",
    "    weights = 'imagenet', \n",
    "    input_shape = (img_height, img_width, 3),\n",
    "    pooling = 'max'\n",
    ")\n",
    "ResNet_base_model.trainable = True\n",
    "\n",
    "# Create a new model on top of the ResNet50 base\n",
    "inputs = tf.keras.Input(shape = (img_height, img_width, 3))\n",
    "x = ResNet_base_model(inputs, training = False)\n",
    "x = BatchNormalization(axis = -1, momentum = 0.99, epsilon = 0.001)(x)\n",
    "x = Dense(256, \n",
    "          kernel_regularizer = regularizers.l2(0.01), \n",
    "          activity_regularizer = regularizers.l1(0.01), \n",
    "          bias_regularizer = regularizers.l1(0.01),\n",
    "          activation = 'relu')(x)\n",
    "x = Dropout(rate = .4, seed = 512)(x)       \n",
    "x = Dense(64, activation = 'relu')(x)\n",
    "outputs = Dense(1, activation = 'sigmoid')(x)\n",
    "ResNet_model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "ResNet_model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adamax(learning_rate = 0.001),\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Summary of the model\n",
    "ResNet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d41b0-af27-4052-adc7-a05fd77da870",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b145062f-cf53-4a6b-bc3b-66eea0b569fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the directory where you want to save the image\n",
    "save_dir = '/home/mahin/Desktop/notebooks/Real and Ai Generated Image Detection'\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Define the full path of the image file\n",
    "save_path = os.path.join(save_dir, 'ResNet_model.png')\n",
    "\n",
    "print(save_path)\n",
    "\n",
    "# Visualize the ResNet model architecture and save the image\n",
    "tf.keras.utils.plot_model(ResNet_model, show_shapes=True, to_file=save_path)\n",
    "\n",
    "# Load and display the saved image\n",
    "image = Image.open('ResNet_model.png')\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6ba866-1009-40ed-8446-23c9c8d7da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ASD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b5d52-5729-4083-9bd8-0dd31ce65358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Transfer Learning model\n",
    "print(\"Starting training with Transfer Learning using ResNet50...\")\n",
    "ResNet_model_history = ResNet_model.fit(\n",
    "    train_ds,\n",
    "    validation_data = val_ds,\n",
    "    epochs = 100,\n",
    "    verbose = 1,\n",
    "    callbacks = [early_stopping]\n",
    ")\n",
    "print(\"Transfer Learning training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a02fe6-e701-452e-be7b-369a0d1319ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaludate the model on the test dataset\n",
    "val_loss, val_accuracy, val_precision, val_recall = ResNet_model.evaluate(val_ds)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Val Loss: {val_loss:.4f}\")\n",
    "print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Val Precision: {val_precision:.4f}\")\n",
    "print(f\"Val Recall: {val_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9610421e-9bbf-4b43-8ce4-849cac107cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error rates and metric rates\n",
    "plot_metrics(ResNet_model_history, 'loss')\n",
    "plot_metrics(ResNet_model_history, 'accuracy')\n",
    "plot_metrics(ResNet_model_history, 'precision')\n",
    "plot_metrics(ResNet_model_history, 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0740eaac-297e-44f7-93d7-be6730fe4f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.plot(range(1, len(train_accuracy) + 1), train_accuracy, label='Training Accuracy')\n",
    "plt.plot(range(1, len(val_accuracy) + 1), val_accuracy, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation loss using a logarithmic scale\n",
    "plt.plot(range(1, len(train_loss) + 1), np.log(train_loss), label='Training Loss')\n",
    "plt.plot(range(1, len(val_loss) + 1), np.log(val_loss), label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('Training and Validation Loss (Log Scale)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create the accuracy plot\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(train_accuracy) + 1), train_accuracy, label='Training Accuracy')\n",
    "plt.plot(range(1, len(val_accuracy) + 1), val_accuracy, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "# Save the accuracy plot as an image\n",
    "plt.savefig('accuracy_plot.png', bbox_inches='tight')\n",
    "\n",
    "# Calculate and display the best validation accuracy and loss\n",
    "best_val_accuracy = max(val_accuracy)\n",
    "best_val_loss = min(val_loss)\n",
    "print(f'Best Validation Accuracy: {best_val_accuracy:.4f}')\n",
    "print(f'Best Validation Loss: {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5832eb-0cd6-4cd5-9cbd-92f3be8e013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define the input shape and number of classes (Make sure to use the same values as in the training)\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 5\n",
    "\n",
    "# Create data generator for testing data\n",
    "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=input_shape[:2],\n",
    "    batch_size=32,\n",
    "    class_mode='sparse',\n",
    "    subset='training',  \n",
    "    shuffle=False  \n",
    ")\n",
    "\n",
    "# Get the true labels for the test data\n",
    "true_labels = test_generator.classes\n",
    "\n",
    "# Get the predictions for the test data\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Calculate and print classification report\n",
    "class_names = list(test_generator.class_indices.keys())\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels, target_names=class_names))\n",
    "\n",
    "# Calculate and plot confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Create an annotation array with the same shape as your confusion matrix\n",
    "annot_array = np.array([['0' if item == 0 else str(item) for item in row] for row in cm])\n",
    "\n",
    "# Create the confusion matrix plot manually\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Set the colormap\n",
    "cmap = plt.cm.Blues\n",
    "\n",
    "# Normalize the confusion matrix data\n",
    "normed_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# This is the colormap I'd like to use.\n",
    "cmap = sns.color_palette(\"Blues\", as_cmap=True)\n",
    "\n",
    "# Plot the heatmap\n",
    "ax.imshow(normed_cm, interpolation='nearest', cmap=cmap)\n",
    "\n",
    "# Add the color bar\n",
    "cbar = ax.figure.colorbar(ax.imshow(normed_cm, interpolation='nearest', cmap=cmap), ax=ax)\n",
    "cbar.ax.set_ylabel('Relative Frequency', rotation=-90, va=\"bottom\")\n",
    "\n",
    "# Add the text annotations.\n",
    "thresh = normed_cm.max() / 1.5\n",
    "for i in range(normed_cm.shape[0]):\n",
    "    for j in range(normed_cm.shape[1]):\n",
    "        ax.text(j, i, annot_array[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if normed_cm[i, j] > thresh else \"black\")\n",
    "\n",
    "# Set the ticks and labels.\n",
    "ax.set_xticks(np.arange(num_classes))\n",
    "ax.set_yticks(np.arange(num_classes))\n",
    "ax.set_xticklabels(class_names)\n",
    "ax.set_yticklabels(class_names)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.set_title('Custom Confusion Matrix')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Get the true and predicted class distributions\n",
    "true_class_distribution = np.bincount(true_labels, minlength=num_classes)\n",
    "predicted_class_distribution = np.bincount(predicted_labels, minlength=num_classes)\n",
    "\n",
    "print(\"True class distribution:\")\n",
    "print(dict(zip(class_names, true_class_distribution)))\n",
    "\n",
    "print(\"Predicted class distribution:\")\n",
    "print(dict(zip(class_names, predicted_class_distribution)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd4559-1477-4d76-854a-ca06154f4da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b2a945-6e82-4c14-b988-590c24d17a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
